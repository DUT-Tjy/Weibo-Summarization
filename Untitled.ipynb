{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from scipy.linalg import norm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=['title', 'article', 'article_cut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name_train = 'F:\\\\NLPCC2015Eval-Task4-AllData\\\\sample data\\\\news.sentences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name_test = 'F:\\\\NLPCC2015Eval-Task4-AllData\\\\TestDataWithReferenceSummaries\\\\news.sentences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prepare(file_name, data_list):\n",
    "    for root, dirs, files in os.walk(file_name):\n",
    "        for file in files:\n",
    "            with open(os.path.join(file_name, file), 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                title = lines[0].strip('\\n')\n",
    "                article = []\n",
    "                for number in range(len(lines[2:])):\n",
    "                    article.append(lines[number+2].strip('\\n'))\n",
    "                row = {\"title\":title, \"article\":article}\n",
    "                data_list = data_list.append(row, ignore_index=True)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data_prepare(file_name_train, data)\n",
    "data = data_prepare(file_name_test, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopwordslist(filepath):\n",
    "    #使用哈工大停用词词典\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seg_sentence(sentence):  # 输出分词后str,分成单个词存储在list中\n",
    "    #使用jieba分词对句子进行切分\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    stopwords = stopwordslist('stopwords.txt')\n",
    "    outstr = ''\n",
    "    out_list = []\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:\n",
    "            if word != \" \":\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "                out_list.append(word)\n",
    "    return outstr, out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算句子与标题相似度\n",
    "def tfidf_similarity(article_list, title):\n",
    "    article_cut = []\n",
    "    title, title_list = seg_sentence(title)\n",
    "    for i in range(len(article_list)):\n",
    "        article, article_list_all = seg_sentence(article_list[i])\n",
    "        article_cut.append(article)\n",
    "    # 转化为TF矩阵\n",
    "    cv = TfidfVectorizer(tokenizer=lambda s: s.split())\n",
    "    article_cut.append(title)\n",
    "    vectors = cv.fit_transform(article_cut).toarray()\n",
    "    similarity_list = []\n",
    "    for num in range(len(article_cut)-1):\n",
    "        similarity_list.append(np.dot(vectors[num], vectors[-1]) / (norm(vectors[num]) * norm(vectors[-1])))\n",
    "    # 计算TF系数\n",
    "    return similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tfidf_similarity(data['article'][0], data['title'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TF_count(article_list, title):\n",
    "    # 构建词表，进行词频统计\n",
    "    article_all = []   #所有词放入一个列表中\n",
    "    article_all_part = [] # 每句话存放在一个list中\n",
    "    title, title_list = seg_sentence(title)\n",
    "    for i in range(len(article_list)):\n",
    "        article, article_cut_list = seg_sentence(article_list[i])\n",
    "        article_all_part.append(article_cut_list)\n",
    "        article_all.extend(article_cut_list)\n",
    "    article_all.extend(title_list)\n",
    "    article_all_part.append(title_list)\n",
    "    counter = Counter(article_all)\n",
    "    # 每句话进行词频统计，获取词频得分\n",
    "    TF_point_list = []\n",
    "    TF_point = 0\n",
    "    for number in range(len(article_list)):\n",
    "        for word in article_all_part[number]:\n",
    "            if word in counter.keys(): \n",
    "                 TF_point += counter[word]/len(article_all)\n",
    "        TF_point_list.append(TF_point)\n",
    "        TF_point = 0\n",
    "    return TF_point_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = TF_count(data['article'][0], data['title'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Doc2vec_train(article_list, title, size=100, epoch_num=1):\n",
    "    # 句子进行jieba分词，按单词存放在list中\n",
    "    article_all_part = []\n",
    "    # 对分词后的句子进行\n",
    "    article_all_part_Tagg = []\n",
    "    title, title_list = seg_sentence(title)\n",
    "    for i in range(len(article_list)):\n",
    "        article, article_cut_list = seg_sentence(article_list[i])\n",
    "        article_all_part.append(article_cut_list)\n",
    "        article = TaggedDocument(article_cut_list, tags=[i])\n",
    "        article_all_part_Tagg.append(article)\n",
    "    article_all_part.append(title_list)\n",
    "    article_all_part_Tagg.append(TaggedDocument(title_list, tags=[len(article_list)]))\n",
    "    print(article_all_part_Tagg)\n",
    "    model = Doc2Vec(article_all_part_Tagg, min_count=1, window=5, size=size, sample=1e-3, negative=5, workers=4)    \n",
    "    model.train(article_all_part_Tagg, total_examples=model.corpus_count, epochs=70)    \n",
    "    inferred_vector_dm = model.infer_vector(article_all_part[-1]) \n",
    "    print(inferred_vector_dm)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['当下', '现实', '语境', '中', '故乡', '语义', '丰富', '模糊', '词语'], tags=[0]), TaggedDocument(words=['一年一度', '春运', '大潮', '显然', '不仅仅', '是因为', '返乡', '动因', '使然'], tags=[1]), TaggedDocument(words=['全球化', '城市化', '进程', '中', '人', '都', '不能不', '面对', '故乡', '异乡', '流转', '变迁', '从未', '离开', '家园', '高速', '发展', '物质', '社会', '中', '不断', '蜕变'], tags=[2]), TaggedDocument(words=['吾', '城吾乡', '传统', '承袭', '故园', '情怀', '重新', '自我', '定位', '背后', '既有', '个人', '生命', '体验', '亦', '家国', '命运', '折射'], tags=[3]), TaggedDocument(words=['选择', '五座', '城乡', '八位', '摄影师', '两', '两', '对照', '持续', '数年', '拍摄', '影像', '阐释', '这一', '命题'], tags=[4]), TaggedDocument(words=['摄影', '一只', '手', '悄悄地', '现实', '精神', '世界', '中', '放下', '地标'], tags=[5]), TaggedDocument(words=['喧嚣', '骚动', '中', '人', '或许', '都', '某种意义', '上', '异乡人', '摄影', '能否', '成为', '微弱', '清晰', '声音', '说', '看', '吾', '城吾乡'], tags=[6]), TaggedDocument(words=['专题', '按语', '李楠', '深圳', '南山区', '南山村', '南园', '村', '北头', '村', '三村', '连成一片', '形成', '深圳', '最大', '城中村'], tags=[7]), TaggedDocument(words=['这是', '透过', '飞机', '舷窗', '拍摄', '三村', '全貌'], tags=[8]), TaggedDocument(words=['2008', '年', '摄'], tags=[9]), TaggedDocument(words=['张', '新民', '图', '吾', '城吾乡', '原乡', '异域', '农村', '包围', '城市', '大', '迁徙', '不仅仅', '是从', '原乡', '异城', '地理', '转移', '更是', '物质', '身份', '精神', '身份', '双重', '置换'], tags=[10]), TaggedDocument(words=['国家', '城市化', '进程', '中', '伴随', '个人', '城市化', '注定', '一场', '身心', '交战'], tags=[11]), TaggedDocument(words=['城市', '栖居', '却', '未必', '归属', '乡土', '无法', '归去', '却', '藕断丝连'], tags=[12]), TaggedDocument(words=['所谓', '寻根', '之旅', '往往', '获得', '表面', '零碎', '记忆', '片断', '之后', '更为', '深刻', '精神', '失落'], tags=[13]), TaggedDocument(words=['父亲', '牵引', '城市', '重返', '原乡', '孙京涛', '只能', '外来者', '目光', '重新', '打量', '最', '熟悉', '人们', '远离', '故里', '张', '新民', '跟随', '乡亲们', '一起', '见证', '中国', '民工潮', '最终', '停靠在', '一座', '平空', '城市', '深圳'], tags=[14]), TaggedDocument(words=['经历', '平静', '镜头', '里', '汹涌', '跌宕起伏', '宿命', '无法', '停止', '寻找'], tags=[15]), TaggedDocument(words=['原乡', '异城', '或许', '将会', '越来越', '难以', '抉择', '矛盾'], tags=[16]), TaggedDocument(words=['沙井镇', '川籍', '农民工', '聚居', '棚户区', '春节', '期间', '很多', '人回', '不了', '故乡', '小', '杂货铺', '电视机', '前', '经常', '挤满', '老乡'], tags=[17]), TaggedDocument(words=['1997', '年', '摄'], tags=[18]), TaggedDocument(words=['张', '新民', '图', '别人', '城市', '腊月', '二十三', '祭', '灶王爷', '老胡', '两口子', '百味', '杂陈', '汽车票', '买到', '深圳', '潼南', '680', '元', '人', '平常', '贵', '一倍', '明天', '终于', '回去', '过年'], tags=[19]), TaggedDocument(words=['深圳', '打工', '10', '年', '两口子', '只', '回去', '两次'], tags=[20]), TaggedDocument(words=['南方', '狭窄', '出租', '屋里', '过年', '攒下', '钱', '帮补', '儿女', '正儿八经', '城里', '安家'], tags=[21]), TaggedDocument(words=['目标', '2013', '年', '就要', '实现', '儿子', '重庆', '买', '房', '春节', '结婚'], tags=[22]), TaggedDocument(words=['喜事', '做', '父母', '当然', '提前', '几天', '回去', '厂里', '硬是', '不', '批假', '放出', '话', '走', '自动', '离职'], tags=[23]), TaggedDocument(words=['意味着', '老胡', '一个月', '工钱', '彻底', '不到'], tags=[24]), TaggedDocument(words=['深圳', '最低工资', '线', '1500', '元', '老胡', '1800', '川籍', '重庆', '籍', '农民工', '深圳', '很多'], tags=[25]), TaggedDocument(words=['出卖', '劳动力', '吃饭', '干', '十几二十年', '从来', '不敢', '想象', '能住', '进', '天价', '房'], tags=[26]), TaggedDocument(words=['住', '棚屋', '住', '城中村', '年复一年', '至今', '未', '变'], tags=[27]), TaggedDocument(words=['1997', '年', '曾', '沙井镇', '拍摄', '老乡', '四川', '农民工', '聚居', '棚户区'], tags=[28]), TaggedDocument(words=['后来', '棚户区', '没有', '外来', '务工人员', '落脚', '地是', '城中村', '城乡', '接合部', '简陋', '出租屋'], tags=[29]), TaggedDocument(words=['二十多年', '过去', '深圳', '城中村', '越来越', '密集', '功能', '越来越', '齐备', '小', '超市', '洗脚', '屋', '网吧', '饭馆', '菜市', '药店', '麻将桌', '老军医', '…', '…', '一应俱全'], tags=[30]), TaggedDocument(words=['住', '地方', '临街', '背后', '三个', '自然村', '连成一片', '巨型', '城中村', '十几万', '租户', '将近', '一半', '操', '四川', '方言', '袭川', '渝', '风俗'], tags=[31]), TaggedDocument(words=['开个', '小店', '忘不了', '腌', '一串', '腊肉', '一棵', '盆栽', '养', '一只', '小狗', '沏', '一壶', '酽', '茶', '四川人', '天性', '丝毫', '不变'], tags=[32]), TaggedDocument(words=['2013', '年', '摄'], tags=[33]), TaggedDocument(words=['张', '新民', '图', '胡榜武', '夫妇', '来自', '潼南县', '农村', '2003', '年来', '深圳', '打工', '至今', '很少', '回老家', '过年'], tags=[34]), TaggedDocument(words=['今年', '儿子', '春节', '结婚', '回家'], tags=[35]), TaggedDocument(words=['老胡', '老家', '照片', '取', '下来', '看'], tags=[36]), TaggedDocument(words=['2013', '年', '摄'], tags=[37]), TaggedDocument(words=['张', '新民', '图', '二十多年', '过去', '特别', '吃苦耐劳', '人', '依然', '不是', '这座', '城市', '主人', '看', '新闻联播', '里', '不断', '刷新', 'GDP', '看路', '虎', '奔驰', '马路上', '风驰电掣', '偶尔', '去', '华强北', '看新起', '地标', '王', '去', '深圳湾', '看', '一水之隔', '香港', '…', '…', '看着', '别人', '城市', '日新月异', '父老乡亲', '一天天', '老去'], tags=[38]), TaggedDocument(words=['人', '慢慢', '淡忘', '老家', '慢慢', '习惯', '出租', '屋里', '城市化'], tags=[39]), TaggedDocument(words=['前海', '建筑工地', '川籍', '农民工', '背上', '刺青', '人生', '最大', '痛苦', '失去', '自由'], tags=[40]), TaggedDocument(words=['1999', '年', '年', '摄'], tags=[41]), TaggedDocument(words=['张', '新民', '图'], tags=[42]), TaggedDocument(words=['吾', '城吾乡', '原乡', '异域', '别人', '城市'], tags=[43])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00364307 -0.00871885 -0.05591159  0.00730042 -0.11123246  0.05568729\n",
      "  0.06112423 -0.05820591 -0.02621388 -0.0331296  -0.09972227 -0.01307449\n",
      " -0.09393863 -0.02301978  0.03170907 -0.07388121  0.02834924 -0.03853439\n",
      " -0.1326794   0.00072193  0.01025284  0.03145637 -0.02512902 -0.01034301\n",
      "  0.18336163 -0.07763074  0.03072446 -0.00255158 -0.16200973 -0.0154889\n",
      " -0.09335517  0.12684824  0.24654053  0.08668745 -0.0284239   0.11337172\n",
      " -0.12139861 -0.03166172  0.02999217 -0.05441087  0.03589702 -0.05319728\n",
      "  0.00913267 -0.06964517 -0.05773924  0.05407128  0.07152052  0.07352702\n",
      " -0.0049653   0.07118604 -0.14689822  0.01514876 -0.12887043  0.09488627\n",
      "  0.06232539  0.04903065 -0.0350323  -0.01939493 -0.07329221 -0.05021406\n",
      " -0.01335816  0.02182754  0.02879551  0.01953697  0.11973416  0.03450743\n",
      " -0.04341694  0.06235024 -0.1115652   0.06580329  0.13365054 -0.0864237\n",
      "  0.0634345   0.11088933  0.00438426  0.11475363  0.00958979 -0.09217833\n",
      "  0.02409441  0.09631451 -0.12011264 -0.01077791  0.0630872   0.00210138\n",
      "  0.11603337  0.01564536 -0.01320025  0.06318998  0.08819763  0.04566862\n",
      " -0.07602851 -0.00750183  0.01933324 -0.07737924  0.03001411 -0.05089852\n",
      " -0.09037397  0.07858476  0.01730522  0.12364962]\n"
     ]
    }
   ],
   "source": [
    "d = Doc2vec_train(data['article'][0], data['title'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
